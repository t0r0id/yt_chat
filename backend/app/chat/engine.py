from app.db.models import Channel, ChannelStatus, Chat, ChatMessage
from click import UUID
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index import VectorStoreIndex
from llama_index.core.llms.types import MessageRole
import os


async def create_new_chat(channel_id: str) -> UUID:
    """
    Create a new chat for the given channel ID.
    
    Args:
    - channel_id (str): The ID of the channel for which the chat is being created.
    
    Returns:
    - str: The ID of the newly created chat.
    
    Raises:
    - ValueError: If the channel is not found or not active, or if failed to create new chat.
    """
    # Get the channel with the specified ID
    channel = await Channel.get(channel_id)
    
    # Check if the channel exists
    if not channel:
        raise ValueError("Channel not found.")
    
    # Check if the channel is active
    if channel.status != ChannelStatus.ACTIVE:
        raise ValueError("Channel is not active.")
    
    # Create a new chat with the channel ID as the vector index name and namespace
    chat = Chat(vector_index_name=os.environ['VECTOR_STORE_INDEX_NAME'],
                vector_namespace=channel.id)
    
    print(chat)
    
    # Save the new chat and return its ID if successful
    if await chat.save():
        return chat.id

    # Raise an error if failed to create new chat
    raise ValueError("Failed to create new chat.")

async def get_chat_history(chat_id: str):
    """
    Retrieve chat history based on the provided chat_id.

    Args:
    chat_id (str): The ID of the chat to retrieve history from.

    Returns:
    str: The chat history.
    
    Raises:
    ValueError: If the chat is not found.
    """
    chat = await Chat.get(chat_id)
    if not chat:
        raise ValueError("Chat not found.")

    return chat.chat_history

async def generate_chat_response(user_message: str, chat_id: str) -> str:
    """
    Generate a response to a user message in a chat.
    
    Args:
    user_message (str): The message sent by the user
    chat_id (str): The ID of the chat
    
    Returns:
    str: The response generated by the chat engine
    """
    
    # Get the chat from the database using the chat_id
    chat = await Chat.get(chat_id)
    
    # If chat is not found, raise a ValueError
    if not chat:
        raise ValueError("Chat not found.")
    
    if not chat.chat_history:
        chat.chat_history = []
    
    # Create an instance of MongoDBAtlasVectorSearch
    vector_store = PineconeVectorStore(api_key=os.environ['PINECONE_API_KEY'],
                                       index_name=chat.vector_index_name,
                                       namespace=chat.vector_namespace,
                                       )
    
    # Create an index from the vector store
    index = VectorStoreIndex.from_vector_store(vector_store)
    
    # Create a query engine from the index
    chat_engine = index.as_chat_engine(chat_mode=chat.chat_mode,
                                       kwargs=chat.chat_kwargs)

    # Query the channel and get the response
    response = await chat_engine.achat(user_message, chat_history= chat.chat_history)

    # Hacky way to convert llamaindex pydantic v1 ChatMessage to pydantic v2 ChatMessage
    chat.chat_history = [ChatMessage(**dict(c)) for c in chat.chat_history]

    # Save the updated chat in the database
    await chat.save()

    # Return the response generated by the chat engine
    return response.response
